# yaml-language-server: $schema=https://raw.githubusercontent.com/asynchronous-flows/asyncflows/main/schemas/asyncflows_schema.json

default_model:
  model: ollama/llama3
flow:
  # Iterate over the PDF filepaths
  extract_pdf_texts:
    for: filepath
    in:
      var: pdf_filepaths
    flow:
      # For each filepath, `extract_pdf_text` extracts text from PDF files
      extractor:
        action: extract_pdf_text
        file:
          var: filepath
  # Analyze the user's query and generate a question for the retrieval system
  generate_query:
    action: prompt
    quote_style: xml
    prompt:
      - heading: User's Message
        var: message
      - text: |
          Carefully analyze the user's message and generate a clear, focused query that captures the key information needed to answer the message. 
          The query should be suitable for a vector search through relevant books.
          Put the query between <query> and </query> tags.
  # Extract the <query>{{query}}</query> from the generated response
  extract_query:
    action: extract_xml_tag
    tag: query
    text:
      link: generate_query.result
  # `retrieve` performs a vector search, fast for large datasets
  retrieval:
    action: retrieve
    k: 20
    documents:
      lambda: |
        [{"title": page.title, "page_num": page.page_number, "text": paragraph}
        for flow in extract_pdf_texts
        for page in flow.extractor.pages
        for paragraph in page.text.split('\n')]  
    texts:
      lambda: |
        [paragraph
        for flow in extract_pdf_texts
        for page in flow.extractor.pages
        for paragraph in page.text.split('\n')] 
    query:
      link: extract_query.result    
  # `rerank` picks the most appropriate documents, it's slower than retrieve, but better at matching against the query
  reranking:
    action: rerank
    k: 5
    documents:
      link: retrieval.result
    texts:
      lambda: |
        [paragraph.text
        for paragraph in retrieval.result]
    query:
      link: extract_query.result
  # # `chatbot` prompts the LLM to summarize the top papers
  # chatbot:
  #   action: prompt
  #   quote_style: xml
  #   prompt:
  #     - role: system
  #     - text: |
  #         You are an expert literary theorist and critic analyzing several Relevant Pages with regards to a New Message. 
  #         Remember what your Conversation History is as you write your response.
  #     - role: user
  #     - heading: Relevant Pages
  #       text: |
  #         {% for page in reranking.result -%}
  #           {{ page.title }}, page number {{ page.page_number }}
  #           ---
  #           {{ page.text }}
  #           ---
  #         {% endfor %}
  #     - heading: New Message
  #       var: message
  #     - text: |
  #         Clearly and concisely respond to the New Message keeping in mind the Relevant Pages and Conversation History if any.
  #         Provide your response to the New Message between <response> and </response> tags.
  # # Extract the <response>{{response}}</response> from the generated response
  # extract_chatbot:
  #   action: extract_xml_tag
  #   tag: response
  #   text:
  #     link: chatbot.result
      
default_output: reranking.result